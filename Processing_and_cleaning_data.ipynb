{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ea8c7e8-9cb3-4bdc-a3aa-2e1aede62c32",
   "metadata": {},
   "source": [
    "# Processing and cleaning data\n",
    "The phases of this part are divided in several sections. Coding styles and techniques vary from one section to fit the specific purpose and according to the skill area of the project member who was writing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f603f01-df28-42fc-8707-b057211283d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "71f8e539-566f-491f-8922-c8dc8f2e515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_dataset = pd.read_stata(\"data/td_ita.dta\")  # time diaries dataset\n",
    "demo_dataset = pd.read_stata(\"data/data4diarynew_ITA.dta\") # demographics dataset\n",
    "stepDetector = pd.read_csv(\"data/stepdetectorevent.csv\") # step counter dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d97602-499c-4fdc-8be6-738dc45b9047",
   "metadata": {},
   "source": [
    "## Base - level cleaning: which users are fit for analysis.\n",
    "An eligible user must not be a drop out, so there is data about their life for every day of the first few weeks of the experiment. \n",
    "For \"total count\" statistical measures other operations are performed. For example, to count how much of the user's day was spent on doing sports, there need to be null answers in the data to signify the times in which a user was inactive that will be summed to the other answers; then we obtain a complete activity dataset where the number of rows doing \"Sports\" equate exactly how much time each user has spent on Sports. \n",
    "\n",
    "**Purpose**: obtaining a list of user IDs that are fit for analysis, and filling missing timestamp data with null answers. \n",
    "\n",
    "**Process**: for each user, an algorithm called \"gantt_data\" checks if the user's data is consistent or not. Only users who pass this test are kept. For each of those, missing data is found and filled with nulle answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "51326fbc-3c53-44ef-92e1-d67caa95752c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 128/128 [00:00<00:00, 740.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At first there were about 241 users. Now: N =  128\n",
      "User dictionary with sorted unique user-specific observation days is done.\n",
      "The test is finished.\n",
      "We end up with: N =  128\n"
     ]
    }
   ],
   "source": [
    "# Users who did sport AT LEAST once will be submitted to this algorithm.\n",
    "\n",
    "\n",
    "\n",
    "time_accurate = td_dataset[(td_dataset.date_not.dt.month == 11 \n",
    "                         ) & (td_dataset.date_not.dt.day >= 13\n",
    "                             ) & (td_dataset.date_not.dt.day <= 30)]\n",
    "\n",
    "once_active = time_accurate[time_accurate['what'] == 'Sport'].id.unique()\n",
    "\n",
    "once_active = time_accurate[time_accurate['what'] == 'Sport'].id.unique()\n",
    "IDs = pd.Series(time_accurate.id.unique()).astype(int) \n",
    "IDs = IDs[IDs.isin(once_active)]\n",
    "\n",
    "user_activity = dict()\n",
    "for ID in tqdm(IDs):\n",
    "    id_act = time_accurate[time_accurate.id == ID]\n",
    "    user_activity[ID] = list(id_act.date_not.dt.day.sort_values().unique())\n",
    "print(f'At first there were about {time_accurate.id.unique().shape[0]} users. Now: N = ',len(user_activity.keys())) \n",
    "print(\"User dictionary with sorted unique user-specific observation days is done.\")\n",
    "\n",
    "def gantt_data_org(k, v):\n",
    "    \n",
    "    ''' \n",
    "    gantt_data is named after the Gantt chart.\n",
    "    S is the Series of days (regardless of when they start).\n",
    "    '''\n",
    "    \n",
    "    tot = 0\n",
    "    start = np.nan\n",
    "    end = np.nan\n",
    "    consecutive = False\n",
    "    \n",
    "    if len(v):  #if it's not empty, it will check for continuity\n",
    "        tot = (pd.to_datetime(v[-1]) - pd.to_datetime(v[0])).days\n",
    "        start, end = v[0], v[-1]\n",
    "        \n",
    "        for i in range(len(v)-1):\n",
    "            v[i]  #current\n",
    "            v[i+1] #next\n",
    "            difference = (pd.to_datetime(v[i+1]) - pd.to_datetime(v[i])).days\n",
    "            # if there is more than two days of difference it means that there was more than one\n",
    "            # day in the data with no observations, and it is unlikely it's consistent data.\n",
    "            if difference >= 2:    \n",
    "                consecutive = False\n",
    "                break\n",
    "            else:\n",
    "                consecutive = True\n",
    "                \n",
    "    return pd.DataFrame([[k, start, end, tot, consecutive]], \n",
    "                        columns=['id', 'start', 'finish', 'tot', 'cons'])\n",
    "\n",
    "gantt_data = pd.DataFrame(columns=[\"id\", \"start\", \"finish\", \"tot\", \"cons\"])\n",
    "\n",
    "for ID, days in user_activity.items():\n",
    "    res = gantt_data_org(ID, days)\n",
    "    gantt_data = pd.concat([gantt_data, res], ignore_index=True)\n",
    "print(\"The test is finished.\")\n",
    "print(\"We end up with: N = \", gantt_data.id.unique().shape[0])\n",
    "to_keep = gantt_data[gantt_data['cons'] == True].id\n",
    "\n",
    "sport_eve = time_accurate[time_accurate.id.isin(to_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "e06c9188-6c6b-4323-b8e0-a9a821f29fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n"
     ]
    }
   ],
   "source": [
    "print(user_activity[0])\n",
    "print(user_activity[1])\n",
    "#and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67dc825-4a64-4a82-b3b7-ca5c54edd37e",
   "metadata": {},
   "source": [
    "Everyone who has done sport once (128 people) are also people who never dropped out from the experiment. They also all have observations ranging from 13th of November to 30th of November, as shown in the dictionary _user activity_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "59eb37fc-e86a-4c76-acc4-364620d63ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial count of rows: 108,800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 128/128 [00:04<00:00, 30.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished. Total count of rows: 110,592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Initial count of rows: {sport_eve.shape[0]:,}')\n",
    "\n",
    "def today_or_yesterday(t): \n",
    "\n",
    "    '''\n",
    "    This function labels the row with the day it belongs to using circadian time. \n",
    "    If an observation falls in the time range from midnight to 5 a.m. it is \n",
    "    classified as belonging to the precedent calendar day as we humans would perceive it. \n",
    "    Why 5 a.m.? It is where more than 99% of the users go to sleep or \n",
    "    wake up - which means that their day ended or started, \n",
    "    and their perception of which day it is has shifted by one.\n",
    "    Otherwise, it's the same day as indicated the timestamp. \n",
    "    '''\n",
    "    if t.hour < 5 and t.day > 13:\n",
    "        return days[t.day - 1]\n",
    "    else:\n",
    "        return days[t.day]\n",
    "\n",
    "days = dict()\n",
    "for i in range(sport_eve.date_not.dt.day.min(), sport_eve.date_not.dt.day.max() + 1):\n",
    "    days[i] = i - sport_eve.date_not.dt.day.min()  \n",
    "\n",
    "min_range = pd.date_range(start=\"2020-11-13 00:00:00\", end=\"2020-11-30 23:59:59\", freq='30T')   \n",
    "#notifications were fired every half hour and such must be the timing between null events\n",
    "results = pd.DataFrame()\n",
    "for user in tqdm(sport_eve.id.unique()):\n",
    "    # each user data is gathered here\n",
    "    subset = sport_eve[sport_eve.id == user]\n",
    "    # an empty dataset is created with range\n",
    "    complete_data = pd.DataFrame({'date_not': min_range})\n",
    "\n",
    "    # the complete dataset gets the subset rows added at its bottom. \n",
    "    merged_df = pd.concat([complete_data, subset])\n",
    "    #the id column, being a costant in each iteration, is filled with the user passed throug the iteration\n",
    "    merged_df['id'] = merged_df['id'].fillna(user)\n",
    "\n",
    "    #the merged dataset gets values sorted: for one timestamp there is a NaN and possibly a real activity from timediary\n",
    "    # here below they are sorted with NaN being put first, and then dropped in the following line\n",
    "    merged_df = merged_df.sort_values(by=\"what\", na_position='first', ascending=True)\n",
    "    merged_df = merged_df.drop_duplicates(keep='last', subset='date_not')\n",
    "\n",
    "    merged_df['what'] = merged_df['what'].fillna('No information')\n",
    "    #merged_df['what'] = merged_df['what'].astype(str)\n",
    "\n",
    "    error = 'Some mistake occurred. \\nThe amount of observation time is different than expected.'\n",
    "    \n",
    "    if merged_df.date_not.min() != complete_data.date_not.min() or merged_df.date_not.max() != complete_data.date_not.max():\n",
    "        print(f\"\\n{error}\\nUser: {user}, dates: {merged_df.date_not.min(), merged_df.date_not.max()}\\n\")\n",
    "    if merged_df.shape[0] != complete_data.shape[0]: \n",
    "        print(f\"\\n{error}\\nUser: {user}, length: {merged_df.shape[0]}, expected: {complete_data.shape[0]}\\n\")\n",
    "    results = pd.concat([results, merged_df], ignore_index=True)\n",
    "results['day'] = [today_or_yesterday(time) for time in results.date_not]\n",
    "results.id = results.id.astype(int)\n",
    "print(f'Finished. Total count of rows: {results.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b933ef8a-c813-4ac0-aeb2-30758b4462eb",
   "metadata": {},
   "source": [
    "Quick math to explain why exactly $110,592$ rows:\n",
    "\n",
    "Each day has 48 half hours - corresponding to the times a notification was fired. \n",
    "Each user has 18 days of observations in the complete dataset. \n",
    "There are 128 users.\n",
    "$48*18*128 =110,592 $\n",
    "\n",
    "The \"complete but empty\" dataset called _complete_data_ is used to attach onto it each user's actual answers. Then, if for the same timestamp there are two observations, the empty one gets dropped to make space for the actual answer. There are only 2,592 differences, because the users did not classify them as some specific activity when they weren't answering the phone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "09cce2e9-ec0d-4008-9e94-4bcce6ac92e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144]\n",
      "\n",
      "Completed dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "what\n",
       "No information                                    26752\n",
       "Sleeping                                          18069\n",
       "Study/work group                                  12036\n",
       "I will go to sleep                                 7388\n",
       "Eating                                             6097\n",
       "Watching TV, video, YouTube, etc.                  5383\n",
       "Lecture/seminar/conference/university meeting      4302\n",
       "Expired                                            3404\n",
       "Personal care                                      2671\n",
       "Did not do anything special                        2271\n",
       "Social life                                        2213\n",
       "Cooking, Food preparation & management             1828\n",
       "Games                                              1692\n",
       "Household and family care                          1628\n",
       "Sport                                              1433\n",
       "Rest/nap                                           1372\n",
       "Phone/Video calling                                1326\n",
       "Walking                                            1227\n",
       "Social media (Facebook Instagram etc.)              990\n",
       "Travelling                                          963\n",
       "Reading a book, periodicals, news, etc.             926\n",
       "Break (coffee, cigarette, drink, etc.)              801\n",
       "Work                                                782\n",
       "Surfed or seeking via Internet                      780\n",
       "Other                                               743\n",
       "In chat on Internet or reading, sending e-mail      490\n",
       "I am starting classes/lessons/lab                   450\n",
       "Listening to music                                  418\n",
       "Hobbies                                             361\n",
       "Grocery Shopping                                    330\n",
       "Voluntary work, and participatory activities        244\n",
       "Happy Hour/Drinking/Party                           225\n",
       "Arts                                                225\n",
       "Other Shopping                                      216\n",
       "Free Time Study                                     209\n",
       "Others                                              133\n",
       "I have a work/study meeting                          79\n",
       "Others Entertainment and Culture                     61\n",
       "I will participate in sports activities              45\n",
       "Entertainment Exhibit, and Culture                   11\n",
       "Movie Theatre Concert ...                            10\n",
       "I am at the cinema/theater/hospital/church            8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MODIFIED DATASET\n",
    "\n",
    "check = results\n",
    "check['day'] = check['date_not'].dt.day \n",
    "total = []\n",
    "for day in check['day'].unique():\n",
    "    total.append(sum(check[check['day']==day][['what']].value_counts().sort_values()))\n",
    "#Further check: each day should have the same amount of observations. \n",
    "print(total)\n",
    "print()\n",
    "print('Completed dataset')\n",
    "check.what.value_counts().sort_values(ascending=False) #you can see that \"No information\" receives more observations than in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "ac4bd8ff-8b9f-41d9-b015-7a55cfd7fdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 4352]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "what\n",
       "No information                                    24960\n",
       "Sleeping                                          18069\n",
       "Study/work group                                  12036\n",
       "I will go to sleep                                 7388\n",
       "Eating                                             6097\n",
       "Watching TV, video, YouTube, etc.                  5383\n",
       "Lecture/seminar/conference/university meeting      4302\n",
       "Expired                                            3404\n",
       "Personal care                                      2671\n",
       "Did not do anything special                        2271\n",
       "Social life                                        2213\n",
       "Cooking, Food preparation & management             1828\n",
       "Games                                              1692\n",
       "Household and family care                          1628\n",
       "Sport                                              1433\n",
       "Rest/nap                                           1372\n",
       "Phone/Video calling                                1326\n",
       "Walking                                            1227\n",
       "Social media (Facebook Instagram etc.)              990\n",
       "Travelling                                          963\n",
       "Reading a book, periodicals, news, etc.             926\n",
       "Break (coffee, cigarette, drink, etc.)              801\n",
       "Work                                                782\n",
       "Surfed or seeking via Internet                      780\n",
       "Other                                               743\n",
       "In chat on Internet or reading, sending e-mail      490\n",
       "I am starting classes/lessons/lab                   450\n",
       "Listening to music                                  418\n",
       "Hobbies                                             361\n",
       "Grocery Shopping                                    330\n",
       "Voluntary work, and participatory activities        244\n",
       "Happy Hour/Drinking/Party                           225\n",
       "Arts                                                225\n",
       "Other Shopping                                      216\n",
       "Free Time Study                                     209\n",
       "Others                                              133\n",
       "I have a work/study meeting                          79\n",
       "Others Entertainment and Culture                     61\n",
       "I will participate in sports activities              45\n",
       "Entertainment Exhibit, and Culture                   11\n",
       "Movie Theatre Concert ...                            10\n",
       "I am at the cinema/theater/hospital/church            8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ORIGINAL DATA\n",
    "check = sport_eve.copy()\n",
    "check['day'] = check['date_not'].dt.day\n",
    "total = []\n",
    "for day in check['day'].unique():\n",
    "    total.append(sum(check[check['day']==day][['what']].value_counts().sort_values()))\n",
    "print(total)\n",
    "check.what.value_counts().sort_values(ascending=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53dd77d-ae5d-4471-93b5-fe72f927342d",
   "metadata": {},
   "source": [
    "### Saving the data for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "c7ed0909-faf1-464d-8861-aee7c5d1d7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4 s ± 15.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "results.to_csv('data/sport_eventformat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb17d8b-5b59-4c77-9302-ef4365ae2c72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sport data - from events to sessions\n",
    "In order to use perform specific analyses, the data must fit specific machine-readable standards. In this section for later purposes (in other notebooks).\n",
    "\n",
    "\n",
    "**Purpose**: obtain duration of sport activity, associations between location and activity, between human company and activity, and finally type of activity. \n",
    "\n",
    "**Process**: to understand the way people have answered \"Sport\", the events of said activity are grouped with each other when they are adjacent. Other code re-categorization are also performed for simplicity. Other techniques are used to model the data in order to obtain the associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "28b123ac-b31d-4ae7-9592-c294b990d895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29026/3884489436.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_td['date_not'] = pd.to_datetime(new_td['date_not'])\n",
      "/tmp/ipykernel_29026/3884489436.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_td['date_dur'] = new_td['date_not'] - timedelta(hours = 5)\n"
     ]
    }
   ],
   "source": [
    "# Session Duration\n",
    "td_cleaned = results[(results[\"what\"]==\"Sport\") | (results[\"what\"]==\"I will participate in sports activities\") |(results[\"what\"]==\"Walking\")]\n",
    "\n",
    "new_td = td_cleaned\n",
    "new_td['date_not'] = pd.to_datetime(new_td['date_not'])\n",
    "new_td['date_dur'] = new_td['date_not'] - timedelta(hours = 5)\n",
    "\n",
    "new_td = new_td.sort_values('date_dur') # sort the dataframe according to \"date_dur\"\n",
    "new_td['time_diff'] = new_td['date_dur'].diff() # compute the time difference between adjacent rows\n",
    "\n",
    "new_session = (new_td['time_diff'] > pd.Timedelta(minutes=30)) | new_td['time_diff'].isnull() # search for new start session\n",
    "new_td['session'] = new_session.cumsum() # Creazione di un nuovo identificatore di sessione basato su new_session\n",
    "\n",
    "collapsed_sessions = new_td.groupby(['id', 'session']).agg(\n",
    "    start_time=('date_not', 'first'),\n",
    "    end_time=('date_not', 'last'),\n",
    "    duration=('date_dur', lambda x: ((x.max() - x.min()).total_seconds() / 60)+30)\n",
    ").reset_index()\n",
    "\n",
    "collapsed_sessions = collapsed_sessions.drop(columns='session')\n",
    "\n",
    "# merge the two datasets\n",
    "td_sessions = td_cleaned.merge(collapsed_sessions, left_on=['id', 'date_not'], right_on=['id', 'start_time']) # inner join of the two datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eefead7-9df9-4a85-adf3-5bb08695a38d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Some skimming\n",
    "In this subsection we evaluate if each session is above 30 minutes of duration (even walking, which may be considered a sport when done above a certain threshold). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "4e4864a5-3c7e-4051-b3c2-f2177abfd795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of cases: 1028\n",
      "Number of cases after first cleaning: 864\n",
      "Number of cases after second cleaning: 864\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initial number of cases: {td_sessions.shape[0]}\")\n",
    "\n",
    "# obtain the indexes of the rows satisfying condition 1\n",
    "to_remove1 = list(td_sessions[(td_sessions['what']=='Walking') & (td_sessions['duration']==30.0)].index)\n",
    "\n",
    "# remove these cases\n",
    "td_sessions = td_sessions.drop(index=to_remove1)\n",
    "print(f\"Number of cases after first cleaning: {td_sessions.shape[0]}\")\n",
    "\n",
    "# obtain the indexes of the rows satisfying condition 2\n",
    "places = [ 'Supermarket …', 'Street markets', 'Grocery Shop', 'University Library', 'Other university place']\n",
    "to_remove2 = list(td_sessions[(td_sessions['what']=='Walking') & (td_sessions['where'].isin(places))].index)\n",
    "\n",
    "# remove these cases\n",
    "td_cleaned = td_sessions.drop(index=to_remove2)\n",
    "print(f\"Number of cases after second cleaning: {td_sessions.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "5ffbdf3c-f00a-4d25-b76d-cdcc624a6ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of sport activity variable\n",
    "td_cleaned['where recoded'] = td_cleaned['where'].replace(['Home apartment /room', 'Weekend home or holiday apartment', 'House (friends others)', 'Relatives Home', 'Home garden/patio/courtyard',\n",
    "                                                           'Another indoor place', 'Gym, swimming pool, Sports centre …', 'Other university place', 'Countryside/mountain/hill/beach', 'In the street', 'Another outdoor place', 'Café, pub, bar', 'Shops, shopping centres', 'Not answer'],\n",
    "                                                          ['indoor', 'indoor', 'indoor', 'indoor', 'indoor',\n",
    "                                                           'outdoor', 'outdoor', 'outdoor', 'outdoor', 'outdoor', 'outdoor', 'outdoor', 'outdoor', 'not specified'])\n",
    "td_cleaned['where recoded'] = td_cleaned['where recoded'].cat.remove_unused_categories()\n",
    "\n",
    "\n",
    "# company during sport activity variable\n",
    "td_cleaned['withw recoded'] = td_cleaned['withw'].replace(['Partner', 'Friend(s)', 'Relative(s)', 'Roommate(s)', 'Other', 'Colleague(s)',\n",
    "                                                           'Classmate(s)',\n",
    "                                                           'Alone',\n",
    "                                                           'Not answer'],\n",
    "                                                          ['company', 'company', 'company', 'company', 'company', 'company', 'company',\n",
    "                                                           'alone',\n",
    "                                                           'not specified'])\n",
    "\n",
    "td_cleaned['withw recoded'] = td_cleaned['withw recoded'].cat.remove_unused_categories()\n",
    "\n",
    "\n",
    "# type of sport activity variable\n",
    "td_cleaned['sport recoded'] = td_cleaned['sport'].replace(['Walking, Trekking, and hiking',\n",
    "                                                           'Jogging and running',\n",
    "                                                           'Cycling, skiing, and skating', 'Ball games', 'Other outdoor activities',\n",
    "                                                           'Gymnastics and fitness',\n",
    "                                                           'Water sports', 'Other indoor activities'\n",
    "                                                           ],\n",
    "                                                          ['Walking, Trekking, and hiking',\n",
    "                                                           'Jogging and running',\n",
    "                                                           'Outdoor activities', 'Outdoor activities', 'Outdoor activities',\n",
    "                                                           'Gymnastics and fitness',\n",
    "                                                           'Other indoor activities', 'Other indoor activities'\n",
    "                                                           ])\n",
    "\n",
    "# adding the category Walking to the sport activity\n",
    "td_cleaned['sport recoded'] = td_cleaned['sport recoded'].cat.add_categories('Walking')\n",
    "td_cleaned.loc[td_cleaned['what'] == 'Walking', 'sport recoded'] = 'Walking'\n",
    "\n",
    "td_cleaned['sport recoded'] = td_cleaned['sport recoded'].cat.remove_unused_categories()\n",
    "\n",
    "# useful conversions\n",
    "td_cleaned['id'] = td_cleaned['id'].astype(int)\n",
    "demo_dataset['userid'] = demo_dataset['userid'].astype(int)\n",
    "td_cleaned['hours'] = td_cleaned['date_not'].dt.hour\n",
    "\n",
    "\n",
    "new_dataset = td_cleaned.copy()\n",
    "new_dataset = new_dataset[['id', 'start_time', 'week', 'DD_not', 'hh_not', 'sport recoded', 'duration', 'where recoded', 'withw recoded']]\n",
    "cat_dataset = new_dataset.merge(demo_dataset, left_on='id', right_on='userid', how='left')\n",
    "\n",
    "cat_dataset = cat_dataset[[ 'sport recoded', 'where recoded', 'withw recoded','degree', 'Dep_UNITN', 'w1_A01']]\n",
    "cat_dataset.columns = ['type of activity', 'location', 'company', 'degree', 'department',  'sex']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51045d2a-d933-49c2-a9df-86c283631a72",
   "metadata": {},
   "source": [
    "### Saving the data for later analysis\n",
    "Data organized in sessions such as above is saved for analysis in other modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "6ec9adb2-90c5-43a8-8ecb-9220f4600f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 ms ± 184 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "td_cleaned.to_csv('data/td_cleaned.csv')\n",
    "cat_dataset.to_csv('data/cat_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2609b1de-8505-476e-b0d4-f93df1f85b34",
   "metadata": {},
   "source": [
    "## Step detector data - from events to sessions\n",
    "**Purpose:** possess knowledge on sensor data - which can be arguably more precise than time diary answers hours after the event. \n",
    "\n",
    "**Process:** the sensor data is in the form of events; every sharp 30 minutes worth of events is aggregated and will result in one row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "538bb087-e766-4a02-b281-5e72ff5d844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stepDetector['timestamp'] = pd.to_datetime(stepDetector['timestamp'], format='%Y%m%d%H%M%S%f')\n",
    "stepDetector1min = stepDetector.copy()\n",
    "stepDetector30min = stepDetector.copy()\n",
    "\n",
    "stepDetector1min['rounded_timestamp'] = stepDetector1min['timestamp'].dt.floor('T')\n",
    "result1m = stepDetector1min.groupby(['userid', 'day', 'rounded_timestamp']).size().reset_index(name=\"step_count\")\n",
    "\n",
    "\n",
    "stepDetector30min['rounded_timestamp'] = stepDetector30min['timestamp'].dt.floor('30T')\n",
    "result30min = stepDetector30min.groupby(['userid', 'day', 'rounded_timestamp']).size().reset_index(name=\"step_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "01927c81-608e-47a6-b9c1-66ce7b907159",
   "metadata": {},
   "outputs": [],
   "source": [
    "result30min = result30min[result30min.rounded_timestamp <= \"2020-11-30 23:59:59\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "764bad46-d5c1-4e55-8503-42bd6c56956b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial count of rows: 22,587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 126/126 [00:00<00:00, 409.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished. Total count of rows: 127,008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Initial count of rows: {result30min.shape[0]:,}')\n",
    "\n",
    "days = dict()\n",
    "for i in range(result30min.rounded_timestamp.dt.day.min(), result30min.rounded_timestamp.dt.day.max() + 1):\n",
    "    days[i] = i - result30min.rounded_timestamp.dt.day.min()  \n",
    "\n",
    "min_range = pd.date_range(start=\"2020-11-10 00:00:00\", end=\"2020-11-30 23:59:59\", freq='30T')   \n",
    "#notifications were fired every half hour and such must be the timing between null events\n",
    "results = pd.DataFrame()\n",
    "for user in tqdm(result30min.userid.unique()):\n",
    "    # each user data is gathered here\n",
    "    subset = result30min[result30min.userid == user]\n",
    "    # an empty dataset is created with range\n",
    "    complete_data = pd.DataFrame({'rounded_timestamp': min_range})\n",
    "\n",
    "    # the complete dataset gets the subset rows added at its bottom. \n",
    "    merged_df = pd.concat([complete_data, subset])\n",
    "    #the id column, being a costant in each iteration, is filled with the user passed throug the iteration\n",
    "    merged_df['userid'] = merged_df['userid'].fillna(user)\n",
    "\n",
    "    #the merged dataset gets values sorted: for one timestamp there is a NaN and possibly a real activity from timediary\n",
    "    # here below they are sorted with NaN being put first, and then dropped in the following line\n",
    "    merged_df = merged_df.sort_values(by=\"step_count\", na_position='first', ascending=True)\n",
    "    merged_df = merged_df.drop_duplicates(keep='last', subset='rounded_timestamp')\n",
    "\n",
    "    merged_df['step_count'] = merged_df['step_count'].fillna(0) #NaN will be replaced with 0. \n",
    "\n",
    "    error = 'Some mistake occurred. \\nThe amount of observation time is different than expected.'\n",
    "    \n",
    "    if merged_df.rounded_timestamp.min() != complete_data.rounded_timestamp.min() or merged_df.rounded_timestamp.max() != complete_data.rounded_timestamp.max():\n",
    "        print(f\"\\n{error}\\nUser: {user}, dates: {merged_df.rounded_timestamp.min(), merged_df.rounded_timestamp.max()}\\n\")\n",
    "    if merged_df.shape[0] != complete_data.shape[0]: \n",
    "        print(f\"\\n{error}\\nUser: {user}, length: {merged_df.shape[0]}, expected: {complete_data.shape[0]}\\n\")\n",
    "    results = pd.concat([results, merged_df], ignore_index=True)\n",
    "    \n",
    "#results['day'] = [today_or_yesterday(time) for time in results.date_not]\n",
    "results.userid = results.userid.astype(int)\n",
    "print(f'Finished. Total count of rows: {results.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f5145-b1e5-4f6e-bbd8-850a54e92c30",
   "metadata": {},
   "source": [
    "### Saving the data for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "45ee837e-a94a-4dde-813d-501a8b6ad53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1m.to_csv(\"data/stepDetector_1min.csv\")\n",
    "result30min.to_csv(\"data/stepDetector_30min.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
